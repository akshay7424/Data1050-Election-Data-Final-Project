{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.8"},"colab":{"name":"ETL_EDA.ipynb","provenance":[{"file_id":"https://github.com/akshay7424/Data1050-Startup-Data-Final-Project/blob/main/Project_Startup_Data.ipynb","timestamp":1605817788732}]}},"cells":[{"cell_type":"markdown","metadata":{"id":"txaiOXpPMAHL"},"source":["## Load Data"]},{"cell_type":"code","metadata":{"id":"N03rd2KOMAHL"},"source":["import pandas as pd\n","import numpy as np\n","import matplotlib\n","from matplotlib import pylab as plt\n","import sklearn\n","from sklearn.model_selection import train_test_split, KFold\n","from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler\n","from sklearn.compose import ColumnTransformer\n","from sklearn.pipeline import Pipeline\n","from sklearn.svm import SVC\n","from sklearn.model_selection import KFold\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.metrics import make_scorer, accuracy_score\n","from sklearn.experimental import enable_iterative_imputer\n","from sklearn.impute import IterativeImputer\n","from sklearn.ensemble import RandomForestRegressor"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"9hHFuOJsMAHL","colab":{"base_uri":"https://localhost:8080/","height":443},"executionInfo":{"status":"error","timestamp":1605913847450,"user_tz":300,"elapsed":2130,"user":{"displayName":"Harry Chalfin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgNQxmGBcCnwbTBwJsbFnZfMi6-TN5y_G7KxMiQ=s64","userId":"06888303268394680574"}},"outputId":"7c07cef6-184f-4569-8fd3-5d212d245150"},"source":["#load data and replace nulls\n","df = pd.read_csv('Startup Data.csv')\n","df = df.replace('NaN', np.nan) \n","\n","#row 124 and 832 are repeats of the same data point\n","df.iloc[124] == df.iloc[832]\n","df = df.drop(832)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-18e636a98a60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#load data and replace nulls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Startup Data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'NaN'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#row 124 and 832 are repeats of the same data point\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Startup Data.csv'"]}]},{"cell_type":"code","metadata":{"id":"pZdboFTsMAHL"},"source":["#check for unique id columns that can be ignored\n","print(df['Unnamed: 0'].value_counts().shape == df['Unnamed: 0'].shape)\n","print(df['id'].value_counts().shape == df['id'].shape)\n","print(df['name'].value_counts().shape == df['name'].shape)\n","print(df['object_id'].value_counts().shape == df['object_id'].shape)\n","\n","#state code is repeated twice with one mismatched state name\n","print((df['state_code.1'] == df['state_code']).value_counts())\n","\n","del df['Unnamed: 0']\n","del df['id']\n","del df['name']\n","del df['object_id']\n","del df['state_code.1']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qlqE1OO0MAHL"},"source":["df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hqgTC4fNMAHM"},"source":["## EDA"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"rlLuAkW1MAHM"},"source":["#histograms/bar charts of all features\n","columns = df.columns\n","for col in columns:\n","    if col in ss_ftrs:\n","        print(df[col].describe())\n","        df[col].plot.hist()\n","        plt.xlabel(col)\n","        plt.ylabel('count')\n","        plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-qnBgTl4MAHM"},"source":["#bar plots"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"LrYG1Db-MAHM"},"source":["#comparing how well california startups did relative to the rest of the country\n","count_matrix = df.groupby(['is_CA', 'status']).size().unstack()\n","count_matrix_norm = count_matrix.div(count_matrix.sum(axis=1),axis=0)\n","count_matrix_norm.plot(kind='bar', stacked=True, color = ['k', 'grey'])\n","plt.xlabel('Outside California (0), In California (1)')\n","plt.ylabel('fraction of startups')\n","plt.legend(loc=4)\n","plt.show()\n","\n","count_matrix = df.groupby(['is_CA', 'has_VC']).size().unstack()\n","count_matrix_norm = count_matrix.div(count_matrix.sum(axis=1),axis=0)\n","count_matrix_norm.plot(kind='bar', stacked=True, color = ['k', 'grey'])\n","plt.xlabel('In California')\n","plt.ylabel('fraction with VC funding')\n","plt.legend(loc=4)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":false,"id":"HhWDijKrMAHM"},"source":["#histogram of the industries\n","df['category_code'].value_counts().plot.bar(color = 'turquoise')\n","plt.ylabel('number of companies')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"p9Oki-0eMAHM"},"source":["#plot showing industry type against total funding amount in usd\n","df.groupby('category_code', as_index=True)['funding_total_usd'].mean().plot.bar(color = 'k')\n","plt.ylabel('total funding')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"fkVVl8RLMAHM"},"source":["#comparing age of first funding year and age at first milestone shows a high concentration near (0,0)\n","#this is likely due to many companies who do not recieve any funding\n","#it is interesting to note the few outliers that were funded much later in their lives\n","df.plot.scatter('age_first_funding_year', 'age_first_milestone_year', s=10, alpha=.1)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"pO_P7NLqMAHM"},"source":["#funding vs status\n","count_matrix = df.groupby(['has_VC', 'status']).size().unstack()\n","count_matrix_norm = count_matrix.div(count_matrix.sum(axis=1),axis=0)\n","count_matrix_norm.plot(kind='bar', stacked=True, color = ['turquoise', 'lavender'])\n","plt.ylabel('status of company')\n","plt.xlabel('VC funding')\n","plt.legend(loc=4)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TctjIuc_MAHM"},"source":["df.groupby(['has_VC', 'status']).size().unstack()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qChMWW6XMAHM"},"source":["## Split Data"]},{"cell_type":"code","metadata":{"id":"iHmuqGK8MAHM"},"source":["#set X and y matrices\n","y = df['status']\n","X = df.loc[:, df.columns != 'status']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j8hzhp1AMAHM"},"source":["#data split\n","random_state = 7\n","\n","#separate out training set\n","X_train, X_other, y_train, y_other = train_test_split(X,y,train_size = 0.6,random_state=random_state)\n","\n","#split out validation and test sets\n","X_val, X_test, y_val, y_test = train_test_split(X,y,train_size = 0.5,random_state=random_state)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OGCXy8rPMAHM"},"source":["## Missing Data"]},{"cell_type":"code","metadata":{"id":"wf2JSq6oMAHM"},"source":["perc_missing_per_ftr = df.isnull().sum(axis=0)/df.shape[0]\n","print('fraction of missing values in features:')\n","print(perc_missing_per_ftr[perc_missing_per_ftr > 0])\n","print('data types of the features with missing values:')\n","print(df[perc_missing_per_ftr[perc_missing_per_ftr > 0].index].dtypes)\n","frac_missing = sum(df.isnull().sum(axis=1)!=0)/df.shape[0]\n","print('fraction of points with missing values:',frac_missing)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oolTZcPwMAHN"},"source":["#categorical missing values new category 'missing'\n","df[\"closed_at\"] = df[\"closed_at\"].replace(np.nan, \"missing\")\n","df[\"Unnamed: 6\"] = df[\"Unnamed: 6\"].replace(np.nan, \"missing\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mV1exP6JMAHN"},"source":["#continuous missing values - multivariate imputation\n","print(df[['age_first_milestone_year','age_last_milestone_year']].head())\n","\n","imputer = IterativeImputer(estimator = RandomForestRegressor(n_estimators=10), random_state=1000)\n","X_impute = imputer.fit_transform(df[['age_first_milestone_year','age_last_milestone_year']])\n","df_imp = pd.DataFrame(data=X_impute, columns = df[['age_first_milestone_year','age_last_milestone_year']])\n","\n","#print(df_train_imp[['LotFrontage','MasVnrArea','GarageYrBlt']].head())\n","\n","#df_CV_imp = pd.DataFrame(data=imputer.transform(df_CV), columns = df_train.columns)\n","#df_test_imp = pd.DataFrame(data=imputer.transform(df_test), columns = df_train.columns)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gFccGWbSMAHN"},"source":["## Preprocessing"]},{"cell_type":"code","metadata":{"id":"D-4fcsqsMAHN"},"source":["#all remaining features split between categorical/continuous/dates to be transformed to continuous\n","#categorical features split further between ordinal scaler and one hot encoder\n","\n","cat_ftrs = ['state_code', 'zip_code', 'city','labels', 'is_CA','is_NY','is_MA', \n","        'is_TX', 'is_otherstate', 'category_code', 'is_software', 'is_web','is_mobile', \n","        'is_enterprise','is_advertising', 'is_gamesvideo', 'is_ecommerce', 'is_biotech', \n","        'is_consulting', 'is_othercategory', 'avg_participants', 'has_VC', 'has_angel', 'has_roundA', \n","        'has_roundB', 'has_roundC', 'has_roundD','is_top500', 'milestones']\n","\n","ss_ftrs = ['latitude', 'longitude', 'age_first_funding_year', 'age_last_funding_year', \n","            'age_first_milestone_year', 'age_last_milestone_year', 'relationships', \n","            'funding_rounds', 'avg_participants', 'funding_total_usd',]\n","\n","dates = ['founded_at', 'closed_at', 'first_funding_at', 'last_funding_at']\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L3KDu3tOMAHN"},"source":["#transform dates into continuous variables using epoch time"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RrB0-V-5MAHN"},"source":["\n","\n","preprocessor = ColumnTransformer(transformers = \n","        [('onehot',  OneHotEncoder(sparse=False, handle_unknown='ignore'), cat_ftrs), \n","          ('ss', StandardScaler(), ss_ftrs)])\n","\n","clf = Pipeline(steps = [('preprocessor', preprocessor)])\n","\n","X_train_prep = clf.fit_transform(X_train)\n","X_val_prep = clf.transform(X_val)\n","X_test_prep = clf.transform(X_test)\n","\n","print(X_train_prep[:5])\n","#for col in cols:\n","   # if col in std_ftrs:\n","       # X_train = scaler.fit_transform(X_train[[col]])\n","\n","\n","#X_train_ohe = enc.fit_transform(X_train[['state_code']])\n","#X_train_ohe"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SJB_4XRUMAHN"},"source":["## Pipeline"]},{"cell_type":"code","metadata":{"id":"QTGmWC4RMAHN"},"source":["def ML_pipeline(X, y, preprocessor, ML_algo, param_grid):\n","    test_scores = []\n","    best_models = []\n","    \n","    for i in range(0,10):\n","        # split data to other/test 80/20, and the use KFold with 4 folds (2 points)\n","        X_other, X_test, y_other, y_test = train_test_split(X, y, test_size=0.2, random_state = 7*i)\n","        kf = KFold(n_splits=4,shuffle=True,random_state=7*i)\n","        # preprocess the data (1 point)\n","        pipe = Pipeline(steps=[('preprocessor', preprocessor), ('regressor', ML_algo)])\n","        # loop through the hyperparameter combinations or use GridSearchCV (2 points)\n","        grid = GridSearchCV(pipe, param_grid=param_grid,scoring = make_scorer(accuracy_score),\n","                        cv=kf, return_train_score = True)\n","        # for each combination, calculate the train and validation scores using the evaluation metric\n","        grid.fit(X_other, y_other)\n","        # find which hyperparameter combination gives the best validation score (1 point)\n","        best_combination = grid.best_params_\n","        # calculate the test score (1 point)\n","        y_pred = grid.predict(X_test)\n","        # append the test score and the best model to the lists (1 point)\n","        test_scores.append(mean_squared_error(y_test, y_pred, squared = False))\n","        best_models.append(grid.best_estimator_)\n","    return best_models, test_scores"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f42-EyszMAHN"},"source":["## Models"]},{"cell_type":"code","metadata":{"id":"ElpQuz9sMAHN"},"source":["ML_algo = SVC()\n","param_grid = {'regressor__C': np.logspace(-3,4,num=8),'regressor__gamma': np.logspace(-3,4,num=8)}\n","ML_pipeline(X, y, preprocessor, ML_algo, param_grid)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CCLVKWa5MAHP"},"source":[""],"execution_count":null,"outputs":[]}]}